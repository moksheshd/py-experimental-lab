{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.parse\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1043407b18371b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "params = {\n",
    "    'checklist_ids': (\n",
    "        # 295059960645029888,\n",
    "        # 297732531005952000,\n",
    "        # 297731055122649088,\n",
    "        279582928276557824,\n",
    "        279123039255269376,\n",
    "        279132491169665024,\n",
    "        279140387039219712,\n",
    "        # 279503535508295680,\n",
    "        # 279912393946959872,\n",
    "        # 279146187807789056,\n",
    "        # 319413730518335488,\n",
    "        # 345194672334168064,\n",
    "        # 341178850955616256,\n",
    "        # 340721427694346240,\n",
    "        # 279537453439651840,\n",
    "        # 279153322457305088,\n",
    "        # 373328040426856448,\n",
    "        # 377706352334987264,\n",
    "        # 279893723157893120,\n",
    "        # 279485800996933632,\n",
    "        # 285400832700059648,\n",
    "        # 404170820620345344,\n",
    "        # 373719698393915392,\n",
    "        # 282732766245687296,\n",
    "        # 279237071089963008,\n",
    "        # 370827644286394368,\n",
    "        # 373051158837977088,\n",
    "        # 398449730782715905,\n",
    "        # 404170305165549568,\n",
    "        # 404170429409222656,\n",
    "        # 398408020304289801,\n",
    "        # 398720454701309965,\n",
    "        # 372709044971233280,\n",
    "        # 398449659236278274,\n",
    "        # 398450587741937677,\n",
    "        # 401966776891957254,\n",
    "        # 398465650968190977,\n",
    "        # 299025913632264192,\n",
    "        # 298049446442176512,\n",
    "        # 404170734775525376,\n",
    "        # 404170246973775872,\n",
    "        # 299026448292777984,\n",
    "        # 298049634288275456,\n",
    "        # 294743731770232832,\n",
    "        # 403800486176055326,\n",
    "        # 404170893622206464,\n",
    "        # 404169905054113792,\n",
    "        # 398685547375583233,\n",
    "        # 294082592220647424,\n",
    "        # 404169756613500928,\n",
    "        # 295059594780086272,\n",
    "        # 403800846068310049,\n",
    "        # 398411306432421890,\n",
    "        # 297709074344054784,\n",
    "        # 291884827033997312,\n",
    "        # 319700013094371328,\n",
    "        # 298051037324906496,\n",
    "        # 319700192111460352,\n",
    "        # 319699919976628224,\n",
    "        # 340720265121357824,\n",
    "        # 341142920697864192,\n",
    "        # 373081645287596032,\n",
    "        # 341147562924171264,\n",
    "        # 341136466985213952,\n",
    "        # 341149343402024960,\n",
    "        # 373051582617870336,\n",
    "        # 341135395411845120,\n",
    "        # 341140342081708032,\n",
    "        # 341148464955383808,\n",
    "        # 398722851980926977,\n",
    "        # 372622984593399808,\n",
    "        # 373081566485012480,\n",
    "        # 372995985339965440,\n",
    "        # 373013004340813824,\n",
    "        # 372982908351537152,\n",
    "        # 366176435520790528,\n",
    "        # 373048691060498432,\n",
    "        # 366172561103773696,\n",
    "        # 366178460010668032,\n",
    "        # 370827261019283456,\n",
    "        # 279116993241550848,\n",
    "        # 279200671120494592,\n",
    "        # 398767965457571842,\n",
    "        # 279147084914888704,\n",
    "        # 404169661704790016,\n",
    "        # 404169976202092544,\n",
    "        # 282735872484315136,\n",
    "        # 404171057044873216,\n",
    "        # 279222687663919104,\n",
    "        # 404170102937182208,\n",
    "        # 279540960947343360,\n",
    "        # 279177979482869760,\n",
    "        # 284575320163487744,\n",
    "        # 373403792681852928,\n",
    "        # 279546007034793984,\n",
    "        # 284920138924298240,\n",
    "        # 284524812233084928,\n",
    "        # 284265201479454720,\n",
    "        # 284517363258232832,\n",
    "        # 284656390242308096,\n",
    "        # 287503238321672192,\n",
    "        # 284651533552848896,\n",
    "        # 286486884764356608,\n",
    "        # 286480488656277504,\n",
    "        # 286489615247872000,\n",
    "        # 286488938941517824,\n",
    "        # 286487406539968512,\n",
    "        # 288977391117668352,\n",
    "        # 288975417454366720,\n",
    "        # 398435421495189507,\n",
    "        # 288945993770078208,\n",
    "        # 288903851307700224,\n",
    "        # 287558216990068736,\n",
    "        # 287552716982456320,\n",
    "        # 404169604909719552,\n",
    "        # 398450308413874177,\n",
    "        # 404171115190509568,\n",
    "        # 294389430942175232,\n",
    "        # 295073696009162752,\n",
    "        # 294320555093397504,\n",
    "        # 299026310887378944,\n",
    "        # 299026674097328128,\n",
    "        # 295078157108645888,\n",
    "        # 403799135358509076,\n",
    "        # 404170951218388992,\n",
    "        # 404169346158911488,\n",
    "        # 279553996445716480,\n",
    "        # 398722153713197073,\n",
    "        # 398454177390878723,\n",
    "        # 404169838180130816,\n",
    "        # 398427215381356554,\n",
    "        # 398724936650678272,\n",
    "        1        \n",
    "    ),\n",
    "    # 'use_case_ids': (1660291903, 1660291904)\n",
    "}\n",
    "\n",
    "db_config = {\n",
    "    'postgres': {\n",
    "        'source': {\n",
    "            'host': 'localhost',\n",
    "            'database': 'tenshi_dwi',\n",
    "            'username': 'postgres',\n",
    "            'password': 'postgres',\n",
    "        },\n",
    "        'destination': {\n",
    "            'host': 'localhost',\n",
    "            'database': 'report_tenshi',\n",
    "            'username': 'postgres',\n",
    "            'password': 'postgres',\n",
    "        },\n",
    "        'jaas': {\n",
    "            'host': 'localhost',\n",
    "            'database': 'tenshi_jaas',\n",
    "            'username': 'postgres',\n",
    "            'password': 'postgres',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "table_names = {\n",
    "    # 'area_cleaning_processes': 'area_cleaning_processes',\n",
    "    'facilities': 'facilities',\n",
    "    'facility_employees': 'facility_employees',\n",
    "    # 'products': 'products',\n",
    "    # 'shifts': 'shifts',\n",
    "    'step_attribute_data_types': 'step_attribute_data_types',\n",
    "    # 'vendors': 'vendors',\n",
    "    # 'area_cleaning_process_stages': 'area_cleaning_process_stages',\n",
    "    # 'facility_locations': 'facility_locations',\n",
    "    # 'manufacturing_processes': 'manufacturing_processes',\n",
    "    'processes': 'processes',\n",
    "    # 'production_batches': 'production_batches',\n",
    "    # 'raw_materials': 'raw_materials',\n",
    "    'stages': 'stages',\n",
    "    'steps': 'steps',\n",
    "    # 'batch_material_usage': 'batch_material_usage',\n",
    "    'equipment': 'equipment',\n",
    "    # 'equipment_cleaning_processes': 'equipment_cleaning_processes',\n",
    "    'executed_steps': 'executed_steps',\n",
    "    # 'manufacturing_stages': 'manufacturing_stages',\n",
    "    # 'product_materials': 'product_materials',\n",
    "    # 'raw_material_lot_details': 'raw_material_lot_details',\n",
    "    'step_attributes': 'step_attributes',\n",
    "    # 'equipment_cleaning_process_stages': 'equipment_cleaning_process_stages',\n",
    "    'executed_step_exceptions': 'executed_step_exceptions',\n",
    "    'executed_step_measurements': 'executed_step_measurements'\n",
    "}\n",
    "\n",
    "if_exists = 'append'\n",
    "display_table = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e0c940ddbce9e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_postgres_connection(postgres_config):\n",
    "    username = urllib.parse.quote_plus(postgres_config['username'])\n",
    "    password = urllib.parse.quote_plus(postgres_config['password'])\n",
    "    return psycopg2.connect(f\"host={postgres_config['host']} dbname={postgres_config['database']} user={username} password={password}\")\n",
    "\n",
    "def get_postgres_engine(postgres_config):\n",
    "    username = urllib.parse.quote_plus(postgres_config['username'])\n",
    "    password = urllib.parse.quote_plus(postgres_config['password'])\n",
    "    connection_string = (f\"postgresql+psycopg2://{username}:{password}@\"\n",
    "                         f\"{postgres_config['host']}/\"\n",
    "                         f\"{postgres_config['database']}\")\n",
    "    return create_engine(connection_string, echo=False)\n",
    "\n",
    "# source_connection = get_postgres_connection(db_config['postgres']['source'])\n",
    "source_engine = get_postgres_engine(db_config['postgres']['source'])\n",
    "destination_connection = get_postgres_connection(db_config['postgres']['destination'])\n",
    "destination_engine = get_postgres_engine(db_config['postgres']['destination'])\n",
    "jaas_engine = get_postgres_engine(db_config['postgres']['jaas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e7977d73c05d25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_id(df: pd.DataFrame, column: str):\n",
    "    max_df_id = df[column].max()\n",
    "    if pd.isna(max_df_id):\n",
    "        return 1\n",
    "    return max_df_id + 1\n",
    "\n",
    "# Function to convert camel case to snake case\n",
    "def camel_to_snake(name):\n",
    "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n",
    "    # Then, remove any non-alphanumeric characters (except underscores)\n",
    "    # by keeping only characters that are alphanumeric or underscores\n",
    "    name = re.sub(r'[^a-zA-Z0-9_]', '', name)\n",
    "    return name\n",
    "\n",
    "def display(df: pd.DataFrame, override=False):\n",
    "    if display_table or override:\n",
    "        print(tabulate(df, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e02f86e12d6be0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FACILITY_QUERY = \"\"\"\n",
    "SELECT id, name FROM facilities WHERE id != -1\n",
    "\"\"\"\n",
    "facility_dtypes =  {'id': 'Int64'}\n",
    "facility_df = pd.read_sql(FACILITY_QUERY, source_engine, params=params, dtype=facility_dtypes)\n",
    "new_facility_df = facility_df.copy()\n",
    "new_facility_df.rename(columns={'id': 'facility_id', 'name': 'facility_name'}, inplace=True)\n",
    "display(new_facility_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new_facility_df.to_sql(table_names['facilities'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74be7160e848052"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USER_QUERY = \"\"\"\n",
    "SELECT u.id, u.first_name, u.last_name, u.employee_id as internal_employee_id, r.\"name\" as role  FROM users u JOIN user_roles_mapping urm ON urm.users_id = u.id JOIN roles r ON urm.roles_id = r.id\n",
    "\"\"\"\n",
    "user_dtypes =  {'id': 'Int64'}\n",
    "user_df = pd.read_sql(USER_QUERY, jaas_engine, params=params, dtype=user_dtypes)\n",
    "new_user_df = user_df.copy()\n",
    "new_user_df.rename(columns={'id': 'employee_id'}, inplace=True)\n",
    "new_user_df.rename(columns={'role': 'employee_role'}, inplace=True)\n",
    "new_user_df['employee_role'] = new_user_df['employee_role'].str.replace('_', ' ').str.title()\n",
    "display(new_user_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3464635d962df474"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new_user_df.to_sql(table_names['facility_employees'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2162e10289c4f54f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16fe9f444545b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHECKLIST_QUERY = \"\"\"\n",
    "SELECT c.id as checklist_id, c.name, c.code, uc.id as use_case_id, uc.name as use_case_name, cfm.facilities_id  \n",
    "FROM checklists c JOIN use_cases uc ON uc.id = c.use_cases_id JOIN checklist_facility_mapping cfm ON cfm.checklists_id = c.id WHERE c.state IN ('PUBLISHED', 'DEPRECATED') AND c.archived = FALSE AND c.id IN %(checklist_ids)s\n",
    "\"\"\"\n",
    "checklist_dtype = {'checklist_id': 'Int64', 'use_case_id': 'Int64', 'facilities_id': 'Int64'}\n",
    "checklist_df = pd.read_sql(CHECKLIST_QUERY, source_engine, params=params, dtype=checklist_dtype)\n",
    "new_process_df = checklist_df.copy()\n",
    "new_process_df.rename(columns={'checklist_id': 'id', 'use_case_name': 'process_type', 'name': 'process_name',\n",
    "                               'facilities_id': 'facility_id'}, inplace=True)\n",
    "new_process_df.drop('code', axis=1, inplace=True)\n",
    "new_process_df.drop('use_case_id', axis=1, inplace=True)\n",
    "display(new_process_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_process_df.to_sql(table_names['processes'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea04e40a8f5fe06d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391675c312c3c87e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STAGE_QUERY = \"\"\"\n",
    "SELECT s.id as stage_id, s.\"name\", s.checklists_id as checklist_id, s.order_tree FROM stages s JOIN checklists c ON c.id = s.checklists_id WHERE s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree\n",
    "\"\"\"\n",
    "stage_dtype = {'stage_id': 'Int64', 'checklist_id': 'Int64'}\n",
    "stage_df = pd.read_sql(STAGE_QUERY, source_engine, params=params, dtype=stage_dtype)\n",
    "display(stage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_stage_df = stage_df.copy()\n",
    "new_stage_df.rename(columns={'stage_id': 'id', 'name': 'stage_name', 'checklist_id': 'process_id'}, inplace=True)\n",
    "new_stage_df.drop('order_tree', axis=1, inplace=True)\n",
    "new_stage_df['stage_type'] = ''\n",
    "display(new_stage_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f8610f6acb7b5ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_stage_df.to_sql(table_names['stages'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5887c0bfdb4a62a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3980352c75c51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TASK_QUERY = \"\"\"\n",
    "SELECT t.id as task_id, t.name, t.order_tree, t.stages_id as stage_id, s.checklists_id as checklist_id FROM tasks t JOIN stages s ON s.id = t.stages_id JOIN checklists c ON c.id = s.checklists_id WHERE t.archived = FALSE AND s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree, t.order_tree \n",
    "\"\"\"\n",
    "task_dtype = {'task_id': 'Int64', 'stage_id': 'Int64', 'checklist_id': 'Int64'}\n",
    "task_df = pd.read_sql(TASK_QUERY, source_engine, params=params, dtype=task_dtype)\n",
    "display(task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PARAMETER_QUERY = \"\"\"\n",
    "SELECT p.id as parameter_id, p.\"label\" AS name, p.\"data\", p.\"type\", p.order_tree, p.tasks_id as task_id, t.stages_id as stage_id, s.checklists_id as checklist_id FROM parameters p JOIN tasks t ON t.id = p.tasks_id JOIN stages s ON s.id = t.stages_id JOIN checklists c ON c.id = s.checklists_id WHERE p.archived = FALSE  AND t.archived = FALSE AND s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree, t.order_tree, p.order_tree\n",
    "\"\"\"\n",
    "parameter_dtype = {'parameter_id': 'Int64', 'task_id': 'Int64', 'stage_id': 'Int64', 'checklist_id': 'Int64'}\n",
    "parameter_df = pd.read_sql(PARAMETER_QUERY, source_engine, params=params, dtype=parameter_dtype)\n",
    "display(parameter_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b3cc941ebbec49d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_step_df = task_df.copy()\n",
    "display(new_step_df)\n",
    "\n",
    "parameter_types = ('INSTRUCTION', 'MATERIAL')\n",
    "# Filter to just instructions \n",
    "instruction_df = parameter_df[parameter_df['type'] == 'INSTRUCTION']\n",
    "\n",
    "# Compile regex once \n",
    "regex = re.compile(r'<.*?>')\n",
    "\n",
    "# Remove HTML tags in a vectorized manner and create a new 'clean_text' column\n",
    "instruction_df['instruction'] = instruction_df['data'].apply(lambda x: re.sub(regex, '', x['text']))\n",
    "\n",
    "# Group by 'task_id' and 'type', then join the texts together\n",
    "grouped = instruction_df.groupby(['task_id', 'type'])['instruction'].apply('\\n'.join).reset_index()\n",
    "\n",
    "# Filter out only the 'INSTRUCTION' type\n",
    "instructions = grouped[grouped['type'] == 'INSTRUCTION']\n",
    "\n",
    "new_step_df = new_step_df.merge(instructions[['task_id', 'instruction']], on='task_id', how='left')\n",
    "\n",
    "new_step_df.rename(columns={'task_id': 'id', 'name': 'step_name'}, inplace=True)\n",
    "new_step_df.drop('order_tree', axis=1, inplace=True)\n",
    "new_step_df.drop('checklist_id', axis=1, inplace=True)\n",
    "\n",
    "display(new_step_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b60271f4d4beb955"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_step_df.to_sql(table_names['steps'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "634fbf6c8f0ac659"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TASK_EXECUTION_QUERY = \"\"\"\n",
    "SELECT te.id AS id, t.id AS task_id, te.reason as reason, TO_TIMESTAMP(te.started_at) AS started_at, TO_TIMESTAMP(te.ended_at) AS ended_at, te.state AS state, te.started_by FROM task_executions te JOIN tasks t ON t.id = te.tasks_id JOIN stages s ON s.id = t.stages_id JOIN checklists c ON c.id = s.checklists_id JOIN jobs j ON j.id = te.jobs_id left join users u ON te.started_by = u.id WHERE t.archived = FALSE AND s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree, t.order_tree\n",
    "\"\"\"\n",
    "task_execution_dtype = {'id': 'Int64', 'task_id': 'Int64', 'started_by': 'Int64'}\n",
    "task_execution_df = pd.read_sql(TASK_EXECUTION_QUERY, source_engine, params=params, dtype=task_execution_dtype)\n",
    "display(task_execution_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fc32dd63bbfde87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PARAMETER_EXECUTION_QUERY = \"\"\"\n",
    "SELECT pv.id, te.id AS task_execution_id, pv.parameters_id, pv.value, pv.choices, p.\"type\" AS parameter_type, to_timestamp(pv.modified_at) AS modified_at, te.tasks_id as task_id FROM parameter_values pv JOIN parameters p ON p.id = pv.parameters_id LEFT JOIN tasks t ON t.id = p.tasks_id LEFT JOIN task_executions te ON te.tasks_id = t.id AND te.jobs_id = pv.jobs_id LEFT JOIN jobs j ON j.id = te.jobs_id AND j.id = pv.jobs_id WHERE p.archived = FALSE AND t.archived = FALSE AND p.type NOT IN ('INSTRUCTION', 'MATERIAL', 'MEDIA', 'SIGNATURE', 'FILE_UPLOAD') AND j.checklists_id IN %(checklist_ids)s\n",
    "\"\"\"\n",
    "parameter_execution_dtype = {'id': 'Int64', 'task_execution_id': 'Int64', 'parameters_id': 'Int64', 'task_id': 'Int64'}\n",
    "parameter_execution_df = pd.read_sql(PARAMETER_EXECUTION_QUERY, source_engine, params=params, dtype=parameter_execution_dtype)\n",
    "display(parameter_execution_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f77f64900c647f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_measurement_df = parameter_execution_df.copy()\n",
    "display(parameter_execution_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e17ace0f5edfd27f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5116e0a86e3e8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_step_attribute_df = pd.DataFrame(\n",
    "    columns=['id', 'step_id', 'data_type_id', 'attribute_label', 'resource_id', 'expected_value1', 'expected_value2',\n",
    "             'comparison_operator', 'resource_type', 'parameter_id', 'reference_id'])\n",
    "\n",
    "new_step_attribute_data_types_df = pd.DataFrame(\n",
    "    columns=['data_type_id', 'measurement_type', 'measurement_unit', 'measurement_description'])\n",
    "\n",
    "def append_to_attribute_related_df(new_rows_attribute_data_types, new_rows_attribute, attribute_data_types_df, attribute_df):\n",
    "    new_rows_data_types_df = pd.DataFrame(new_rows_attribute_data_types)\n",
    "    new_rows_attribute_df = pd.DataFrame(new_rows_attribute)\n",
    "    attribute_data_types_df = pd.concat([attribute_data_types_df, new_rows_data_types_df], ignore_index=True)\n",
    "    attribute_df = pd.concat([attribute_df, new_rows_attribute_df], ignore_index=True)\n",
    "    return  attribute_data_types_df, attribute_df\n",
    "\n",
    "def create_row_attribute_data_type(data_type_id, measurement_type, measurement_unit, measurement_description):\n",
    "    return {\n",
    "        'data_type_id': data_type_id,\n",
    "        'measurement_type': measurement_type,\n",
    "        'measurement_unit': measurement_unit,\n",
    "        'measurement_description': measurement_description\n",
    "    }\n",
    "\n",
    "def create_row_attribute(attribute_id, parameter_id, step_id, data_type_id, attribute_label, expected_value1, expected_value2, comparison_operator, resource_id, resource_type, reference_id):\n",
    "    return {\n",
    "        'id': attribute_id,\n",
    "        'step_id': step_id,\n",
    "        'data_type_id': data_type_id,\n",
    "        'attribute_label': attribute_label,\n",
    "        'expected_value1': expected_value1,\n",
    "        'expected_value2': expected_value2,\n",
    "        'comparison_operator': comparison_operator,\n",
    "        'resource_id': resource_id,\n",
    "        'resource_type': resource_type,\n",
    "        'parameter_id': parameter_id,\n",
    "        'reference_id': reference_id\n",
    "    }\n",
    "\n",
    "def create_rows_for_attribute_related_data(parameter, identifier):\n",
    "    new_rows_attribute_data_type = []\n",
    "    new_rows_attribute = []\n",
    "    measurement_unit = expected_value1 = expected_value2 = comparison_operator = resource_id = resource_type = reference_id = measurement_type = None\n",
    "    parameter_type, step_id, name = parameter['type'], parameter['task_id'], parameter['name']\n",
    "    if parameter_type not in (['SINGLE_SELECT', 'CHECKLIST', 'MULTISELECT', 'YES_NO']):\n",
    "        is_parameter_type_handled = True\n",
    "        if parameter_type == 'NUMBER' or 'CALCULATION':\n",
    "            measurement_type = 'integer'\n",
    "        elif parameter_type == 'SHOULD_BE':\n",
    "            measurement_type = 'float'\n",
    "            operator = parameter['data']['operator']\n",
    "            if operator == 'EQUAL_TO':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '='\n",
    "            elif operator == 'LESS_THAN':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '<'\n",
    "            elif operator == 'LESS_THAN_EQUAL_TO':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '<='\n",
    "            elif operator == 'MORE_THAN':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '>'\n",
    "            elif operator == 'MORE_THAN_EQUAL_TO':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '>='\n",
    "            elif operator == 'BETWEEN':                \n",
    "                expected_value1 = parameter['data']['lowerValue']\n",
    "                expected_value2 = parameter['data']['upperValue']\n",
    "                comparison_operator = 'between'\n",
    "        elif parameter_type == 'SINGLE_LINE' or parameter_type == 'MULTI_LINE':\n",
    "            measurement_type = 'text'\n",
    "        elif parameter_type == 'DATE' or parameter_type == 'DATE_TIME':\n",
    "            measurement_type = parameter_type.lower()\n",
    "        elif parameter_type == 'RESOURCE':\n",
    "            measurement_type = 'text'\n",
    "            resource_type = camel_to_snake(parameter['data']['collection'])\n",
    "        else:\n",
    "            print(f\"Parameter type: {parameter_type} is not implemented\")\n",
    "            is_parameter_type_handled = False\n",
    "        if is_parameter_type_handled:    \n",
    "            new_row_attribute_data_type = create_row_attribute_data_type(identifier, measurement_type, measurement_unit, name)\n",
    "            new_row_attribute = create_row_attribute(identifier, parameter['parameter_id'], step_id, new_row_attribute_data_type['data_type_id'], name, expected_value1, expected_value2, comparison_operator, resource_id, resource_type, reference_id)\n",
    "            new_rows_attribute_data_type.append(new_row_attribute_data_type)\n",
    "            new_rows_attribute.append(new_row_attribute)\n",
    "    else:\n",
    "        if parameter_type in (['SINGLE_SELECT', 'CHECKLIST', 'MULTISELECT', 'YES_NO']):\n",
    "            measurement_type = 'boolean'\n",
    "            for choice in row['data']:\n",
    "                name = parameter['name'] + ' - ' + choice['name']\n",
    "                reference_id = choice['id']\n",
    "                new_row_attribute_data_type = create_row_attribute_data_type(identifier, measurement_type, measurement_unit, name)\n",
    "                new_row_attribute = create_row_attribute(identifier, parameter['parameter_id'], step_id, new_row_attribute_data_type['data_type_id'], name, expected_value1, expected_value2, comparison_operator, resource_id, resource_type, reference_id)\n",
    "                new_rows_attribute_data_type.append(new_row_attribute_data_type)\n",
    "                new_rows_attribute.append(new_row_attribute)\n",
    "                identifier += 1\n",
    "        else:\n",
    "            print(f\"Parameter type: {parameter_type} is not implemented.\")\n",
    "                \n",
    "    return new_rows_attribute_data_type, new_rows_attribute\n",
    "\n",
    "# next_id = get_next_id(new_step_attribute_data_types_df, 'data_type_id')\n",
    "cur = destination_connection.cursor()\n",
    "cur.execute(\"SELECT MAX(data_type_id) FROM step_attribute_data_types\")\n",
    "max_value = cur.fetchone()[0]\n",
    "cur.close()\n",
    "if max_value is None:\n",
    "    max_value = 0\n",
    "next_id = max_value + 1\n",
    "relevant_parameter_df = parameter_df[~parameter_df['type'].isin(['INSTRUCTION', 'MATERIAL', 'MEDIA', 'SIGNATURE', 'FILE_UPLOAD'])]\n",
    "# relevant_parameter_df.rename(columns={'parameter_id' : 'id'}, inplace=True)\n",
    "for index, row in relevant_parameter_df.iterrows():\n",
    "    attribute_data_types, attributes = create_rows_for_attribute_related_data(row, next_id)\n",
    "    if len(attribute_data_types) != 0 and len(attributes) != 0:\n",
    "        new_step_attribute_data_types_df, new_step_attribute_df = append_to_attribute_related_df(attribute_data_types, attributes, new_step_attribute_data_types_df, new_step_attribute_df)\n",
    "    next_id += len(attribute_data_types)\n",
    "\n",
    "# display(new_step_attribute_data_types_df)\n",
    "display(new_step_attribute_df)\n",
    "# new_step_attribute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_step_attribute_data_types_df.to_sql(table_names['step_attribute_data_types'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "587019755e42c011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_step_attribute_df.to_sql(table_names['step_attributes'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2e285ab55369432"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_df = task_execution_df.copy()\n",
    "executed_step_df.rename(columns={'id': 'execution_id', 'task_id': 'step_id', 'started_at': 'execution_start_time', 'ended_at': 'execution_end_time', 'state': 'status', 'started_by': 'executed_by_employee_id'  }, inplace=True)\n",
    "executed_step_df['batch_id'] = None\n",
    "executed_step_df.drop('reason', axis=1, inplace=True)\n",
    "display(executed_step_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48804d634e868135"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_df.to_sql(table_names['executed_steps'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "105cdb66ec2d35cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_exception_df = task_execution_df.copy()\n",
    "executed_step_exception_df = executed_step_exception_df[executed_step_exception_df['state'] == 'COMPLETED_WITH_EXCEPTION']\n",
    "executed_step_exception_df['exception_id'] = executed_step_exception_df['id']\n",
    "executed_step_exception_df.rename(columns={'id': 'execution_id', 'reason': 'description', 'ended_at': 'exception_time'}, inplace=True)\n",
    "executed_step_exception_df.drop('task_id', axis=1, inplace=True)\n",
    "executed_step_exception_df.drop('state', axis=1, inplace=True)\n",
    "executed_step_exception_df.drop('started_at', axis=1, inplace=True)\n",
    "executed_step_exception_df.drop('started_by', axis=1, inplace=True)\n",
    "display(executed_step_exception_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "836979f00f7a3848"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_exception_df.to_sql(table_names['executed_step_exceptions'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "882abfab227fcaaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_measurement_df = pd.DataFrame(columns=['execution_id', 'step_attribute_id', 'resource_id', 'measurement_value', 'recorded_time', 'resource_type'])\n",
    "\n",
    "def create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type): \n",
    "    return {\n",
    "        # 'measurement_id': measurement_id,\n",
    "        'execution_id': execution_id,\n",
    "        'step_attribute_id': step_attribute_id,\n",
    "        'resource_id': resource_id,\n",
    "        'measurement_value': measurement_value,\n",
    "        'recorded_time': recorded_time,\n",
    "        'resource_type': resource_type\n",
    "    }    \n",
    "\n",
    "new_rows_executed_step_measurement = []\n",
    "# relevant_parameter_df = parameter_execution_df[~parameter_execution_df['parameter_type'].isin(['INSTRUCTION', 'MATERIAL', 'MEDIA', 'SIGNATURE', 'FILE_UPLOAD'])]\n",
    "for index, row in parameter_execution_df.iterrows():\n",
    "    parameter_type = row['parameter_type']\n",
    "    measurement_id = row['id']\n",
    "    # measurement_id = None\n",
    "    execution_id = row['task_execution_id']\n",
    "    recorded_time = row['modified_at']\n",
    "    step_id = row['task_id']\n",
    "    parameters_id = row['parameters_id']\n",
    "    step_attribute_id = resource_id = measurement_value = resource_type = None\n",
    "    if parameter_type in (['SINGLE_SELECT', 'CHECKLIST', 'MULTISELECT', 'YES_NO', 'RESOURCE']):\n",
    "        if parameter_type == 'RESOURCE':\n",
    "            choices = row['choices']\n",
    "            if choices is not None:\n",
    "                choice = choices[0]\n",
    "                resource_id = choice['objectId']\n",
    "                resource_type =  camel_to_snake(choice['collection'])\n",
    "                measurement_value = choice['objectDisplayName'] + ' (ID: ' + choice['objectExternalId'] + ')'\n",
    "                new_row_executed_step_measurement = create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type)\n",
    "                new_rows_executed_step_measurement.append(new_row_executed_step_measurement)\n",
    "        else:\n",
    "            if row['choices'] is not None:\n",
    "                for choice_id, selection in row['choices'].items():\n",
    "                    filtered_series = new_step_attribute_df.loc[(new_step_attribute_df['parameter_id'] == parameters_id) & (new_step_attribute_df['reference_id'] == choice_id)]['id']\n",
    "                    if not filtered_series.empty:\n",
    "                        step_attribute_id = filtered_series.iloc[0]\n",
    "                    else:\n",
    "                        print(f\"Not able to find parameter_id, parameters_id: {parameters_id}, parameter_type: {parameter_type}, row: {row}\")\n",
    "                        step_attribute_id = None\n",
    "                    measurement_value = 'false'\n",
    "                    if selection == 'SELECTED':\n",
    "                        measurement_value = 'true'\n",
    "                    new_row_executed_step_measurement = create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type)\n",
    "                    new_rows_executed_step_measurement.append(new_row_executed_step_measurement)\n",
    "    else:\n",
    "        filtered_series = new_step_attribute_df.loc[(new_step_attribute_df['parameter_id'] == parameters_id)]['id']\n",
    "        if not filtered_series.empty:\n",
    "            step_attribute_id = filtered_series.iloc[0]\n",
    "        else:\n",
    "            print(f\"Not able to find parameter_id, parameters_id: {parameters_id}, parameter_type: {parameter_type}, row: {row}\")\n",
    "            step_attribute_id = None\n",
    "        measurement_value = row['value']\n",
    "        new_row_executed_step_measurement = create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type)\n",
    "        new_rows_executed_step_measurement.append(new_row_executed_step_measurement)\n",
    "\n",
    "if len(new_rows_executed_step_measurement) != 0:\n",
    "    new_rows_executed_step_measurement_df = pd.DataFrame(new_rows_executed_step_measurement)\n",
    "    executed_step_measurement_df = pd.concat([executed_step_measurement_df, new_rows_executed_step_measurement_df], ignore_index=True)\n",
    "#             \n",
    "# executed_step_measurement_df\n",
    "# new_rows_executed_step_measurement\n",
    "display(executed_step_measurement_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "347d860a3dbf7242"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "executed_step_measurement_df.to_sql(table_names['executed_step_measurements'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1fdd16355ce7b92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
