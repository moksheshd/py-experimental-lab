{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.843828742Z",
     "start_time": "2023-10-23T05:44:40.152527164Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.parse\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2e1043407b18371b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844047136Z",
     "start_time": "2023-10-23T05:44:40.193437700Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "params = {\n",
    "    'checklist_ids': (\n",
    "        # 295059960645029888,\n",
    "        # 297732531005952000,\n",
    "        # 372622984593399808,\n",
    "        # 372709044971233280,\n",
    "        # 372982908351537152,\n",
    "        # 372995985339965440,\n",
    "        # 373013004340813824,\n",
    "        # 373048691060498432,\n",
    "        # 373051158837977088,\n",
    "        # 373081566485012480,\n",
    "        # 373081645287596032,\n",
    "        # 373328040426856448,\n",
    "        # 373403792681852928,\n",
    "        403800486176055326,\n",
    "        1        \n",
    "    ),\n",
    "    'use_case_ids': (1660291903, 1660291904)\n",
    "}\n",
    "\n",
    "db_config = {\n",
    "    'postgres': {\n",
    "        'source': {\n",
    "            'host': 'localhost',\n",
    "            'database': 'tenshi_dwi',\n",
    "            'username': 'postgres',\n",
    "            'password': 'postgres',\n",
    "        },\n",
    "        'destination': {\n",
    "            'host': 'localhost',\n",
    "            'database': 'report_tenshi',\n",
    "            'username': 'postgres',\n",
    "            'password': 'postgres',\n",
    "        },\n",
    "        'jaas': {\n",
    "            'host': 'localhost',\n",
    "            'database': 'tenshi_jaas',\n",
    "            'username': 'postgres',\n",
    "            'password': 'postgres',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "table_names = {\n",
    "    # 'area_cleaning_processes': 'area_cleaning_processes',\n",
    "    'facilities': 'facilities',\n",
    "    'facility_employees': 'facility_employees',\n",
    "    # 'products': 'products',\n",
    "    # 'shifts': 'shifts',\n",
    "    'step_attribute_data_types': 'step_attribute_data_types',\n",
    "    # 'vendors': 'vendors',\n",
    "    # 'area_cleaning_process_stages': 'area_cleaning_process_stages',\n",
    "    # 'facility_locations': 'facility_locations',\n",
    "    # 'manufacturing_processes': 'manufacturing_processes',\n",
    "    'processes': 'processes',\n",
    "    # 'production_batches': 'production_batches',\n",
    "    # 'raw_materials': 'raw_materials',\n",
    "    'stages': 'stages',\n",
    "    'steps': 'steps',\n",
    "    # 'batch_material_usage': 'batch_material_usage',\n",
    "    'equipment': 'equipment',\n",
    "    # 'equipment_cleaning_processes': 'equipment_cleaning_processes',\n",
    "    'executed_steps': 'executed_steps',\n",
    "    # 'manufacturing_stages': 'manufacturing_stages',\n",
    "    # 'product_materials': 'product_materials',\n",
    "    # 'raw_material_lot_details': 'raw_material_lot_details',\n",
    "    'step_attributes': 'step_attributes',\n",
    "    # 'equipment_cleaning_process_stages': 'equipment_cleaning_process_stages',\n",
    "    'executed_step_exceptions': 'executed_step_exceptions',\n",
    "    'executed_step_measurements': 'executed_step_measurements'\n",
    "}\n",
    "\n",
    "if_exists = 'append'\n",
    "display_table = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a45e0c940ddbce9e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844223132Z",
     "start_time": "2023-10-23T05:44:40.193583357Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_postgres_connection(postgres_config):\n",
    "    username = urllib.parse.quote_plus(postgres_config['username'])\n",
    "    password = urllib.parse.quote_plus(postgres_config['password'])\n",
    "    return psycopg2.connect(f\"host={postgres_config['host']} dbname={postgres_config['database']} user={username} password={password}\")\n",
    "\n",
    "def get_postgres_engine(postgres_config):\n",
    "    username = urllib.parse.quote_plus(postgres_config['username'])\n",
    "    password = urllib.parse.quote_plus(postgres_config['password'])\n",
    "    connection_string = (f\"postgresql+psycopg2://{username}:{password}@\"\n",
    "                         f\"{postgres_config['host']}/\"\n",
    "                         f\"{postgres_config['database']}\")\n",
    "    return create_engine(connection_string, echo=False)\n",
    "\n",
    "# source_connection = get_postgres_connection(db_config['postgres']['source'])\n",
    "source_engine = get_postgres_engine(db_config['postgres']['source'])\n",
    "destination_connection = get_postgres_connection(db_config['postgres']['destination'])\n",
    "destination_engine = get_postgres_engine(db_config['postgres']['destination'])\n",
    "jaas_engine = get_postgres_engine(db_config['postgres']['jaas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e4e7977d73c05d25",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844403978Z",
     "start_time": "2023-10-23T05:44:40.193676089Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_id(df: pd.DataFrame, column: str):\n",
    "    max_df_id = df[column].max()\n",
    "    if pd.isna(max_df_id):\n",
    "        return 1\n",
    "    return max_df_id + 1\n",
    "\n",
    "def display(df: pd.DataFrame, override=False):\n",
    "    if display_table or override:\n",
    "        print(tabulate(df, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a5e02f86e12d6be0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844541010Z",
     "start_time": "2023-10-23T05:44:40.193758989Z"
    }
   },
   "outputs": [],
   "source": [
    "FACILITY_QUERY = \"\"\"\n",
    "SELECT id, name FROM facilities WHERE id != -1\n",
    "\"\"\"\n",
    "facility_dtypes =  {'id': 'Int64'}\n",
    "facility_df = pd.read_sql(FACILITY_QUERY, source_engine, params=params, dtype=facility_dtypes)\n",
    "new_facility_df = facility_df.copy()\n",
    "new_facility_df.rename(columns={'id': 'facility_id', 'name': 'facility_name'}, inplace=True)\n",
    "display(new_facility_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "# new_facility_df.to_sql(table_names['facilities'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844662548Z",
     "start_time": "2023-10-23T05:44:40.207442317Z"
    }
   },
   "id": "74be7160e848052"
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [],
   "source": [
    "USER_QUERY = \"\"\"\n",
    "SELECT u.id, u.first_name, u.last_name, u.employee_id as internal_employee_id, r.\"name\" as role  FROM users u JOIN user_roles_mapping urm ON urm.users_id = u.id JOIN roles r ON urm.roles_id = r.id\n",
    "\"\"\"\n",
    "user_dtypes =  {'id': 'Int64'}\n",
    "user_df = pd.read_sql(USER_QUERY, jaas_engine, params=params, dtype=user_dtypes)\n",
    "new_user_df = user_df.copy()\n",
    "new_user_df.rename(columns={'id': 'employee_id'}, inplace=True)\n",
    "new_user_df.rename(columns={'role': 'employee_role'}, inplace=True)\n",
    "new_user_df['employee_role'] = new_user_df['employee_role'].str.replace('_', ' ').str.title()\n",
    "display(new_user_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844826572Z",
     "start_time": "2023-10-23T05:44:40.211660267Z"
    }
   },
   "id": "3464635d962df474"
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "# new_user_df.to_sql(table_names['facility_employees'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.844991624Z",
     "start_time": "2023-10-23T05:44:40.231057899Z"
    }
   },
   "id": "2162e10289c4f54f"
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "cd16fe9f444545b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.845165022Z",
     "start_time": "2023-10-23T05:44:40.234460552Z"
    }
   },
   "outputs": [],
   "source": [
    "CHECKLIST_QUERY = \"\"\"\n",
    "SELECT c.id as checklist_id, c.name, c.code, uc.id as use_case_id, uc.name as use_case_name, cfm.facilities_id  \n",
    "FROM checklists c JOIN use_cases uc ON uc.id = c.use_cases_id JOIN checklist_facility_mapping cfm ON cfm.checklists_id = c.id WHERE c.state = 'PUBLISHED' AND c.archived = FALSE AND c.use_cases_id IN %(use_case_ids)s AND c.id IN %(checklist_ids)s\n",
    "\"\"\"\n",
    "checklist_dtype = {'checklist_id': 'Int64', 'use_case_id': 'Int64', 'facilities_id': 'Int64'}\n",
    "checklist_df = pd.read_sql(CHECKLIST_QUERY, source_engine, params=params, dtype=checklist_dtype)\n",
    "new_process_df = checklist_df.copy()\n",
    "new_process_df.rename(columns={'checklist_id': 'id', 'use_case_name': 'process_type', 'name': 'process_name',\n",
    "                               'facilities_id': 'facility_id'}, inplace=True)\n",
    "new_process_df.drop('code', axis=1, inplace=True)\n",
    "new_process_df.drop('use_case_id', axis=1, inplace=True)\n",
    "display(new_process_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_process_df.to_sql(table_names['processes'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.845550851Z",
     "start_time": "2023-10-23T05:44:40.277394359Z"
    }
   },
   "id": "ea04e40a8f5fe06d"
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "391675c312c3c87e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.845709736Z",
     "start_time": "2023-10-23T05:44:40.277564968Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE_QUERY = \"\"\"\n",
    "SELECT s.id as stage_id, s.\"name\", s.checklists_id as checklist_id, s.order_tree FROM stages s JOIN checklists c ON c.id = s.checklists_id WHERE s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree\n",
    "\"\"\"\n",
    "stage_dtype = {'stage_id': 'Int64', 'checklist_id': 'Int64'}\n",
    "stage_df = pd.read_sql(STAGE_QUERY, source_engine, params=params, dtype=stage_dtype)\n",
    "display(stage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "new_stage_df = stage_df.copy()\n",
    "new_stage_df.rename(columns={'stage_id': 'id', 'name': 'stage_name', 'checklist_id': 'process_id'}, inplace=True)\n",
    "new_stage_df.drop('order_tree', axis=1, inplace=True)\n",
    "new_stage_df['stage_type'] = ''\n",
    "display(new_stage_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.845880973Z",
     "start_time": "2023-10-23T05:44:40.277641333Z"
    }
   },
   "id": "3f8610f6acb7b5ed"
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stage_df.to_sql(table_names['stages'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.846164903Z",
     "start_time": "2023-10-23T05:44:40.321416830Z"
    }
   },
   "id": "5887c0bfdb4a62a6"
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "49c3980352c75c51",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.846273068Z",
     "start_time": "2023-10-23T05:44:40.321603886Z"
    }
   },
   "outputs": [],
   "source": [
    "TASK_QUERY = \"\"\"\n",
    "SELECT t.id as task_id, t.name, t.order_tree, t.stages_id as stage_id, s.checklists_id as checklist_id FROM tasks t JOIN stages s ON s.id = t.stages_id JOIN checklists c ON c.id = s.checklists_id WHERE t.archived = FALSE AND s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree, t.order_tree \n",
    "\"\"\"\n",
    "task_dtype = {'task_id': 'Int64', 'stage_id': 'Int64', 'checklist_id': 'Int64'}\n",
    "task_df = pd.read_sql(TASK_QUERY, source_engine, params=params, dtype=task_dtype)\n",
    "display(task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "PARAMETER_QUERY = \"\"\"\n",
    "SELECT p.id as parameter_id, p.\"label\" AS name, p.\"data\", p.\"type\", p.order_tree, p.tasks_id as task_id, t.stages_id as stage_id, s.checklists_id as checklist_id FROM parameters p JOIN tasks t ON t.id = p.tasks_id JOIN stages s ON s.id = t.stages_id JOIN checklists c ON c.id = s.checklists_id WHERE p.archived = FALSE  AND t.archived = FALSE AND s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree, t.order_tree, p.order_tree\n",
    "\"\"\"\n",
    "parameter_dtype = {'parameter_id': 'Int64', 'task_id': 'Int64', 'stage_id': 'Int64', 'checklist_id': 'Int64'}\n",
    "parameter_df = pd.read_sql(PARAMETER_QUERY, source_engine, params=params, dtype=parameter_dtype)\n",
    "display(parameter_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.846367891Z",
     "start_time": "2023-10-23T05:44:40.321698109Z"
    }
   },
   "id": "7b3cc941ebbec49d"
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18013/1833256597.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  instruction_df['instruction'] = instruction_df['data'].apply(lambda x: re.sub(regex, '', x['text']))\n"
     ]
    }
   ],
   "source": [
    "new_step_df = task_df.copy()\n",
    "display(new_step_df)\n",
    "\n",
    "parameter_types = ('INSTRUCTION', 'MATERIAL')\n",
    "# Filter to just instructions \n",
    "instruction_df = parameter_df[parameter_df['type'] == 'INSTRUCTION']\n",
    "\n",
    "# Compile regex once \n",
    "regex = re.compile(r'<.*?>')\n",
    "\n",
    "# Remove HTML tags in a vectorized manner and create a new 'clean_text' column\n",
    "instruction_df['instruction'] = instruction_df['data'].apply(lambda x: re.sub(regex, '', x['text']))\n",
    "\n",
    "# Group by 'task_id' and 'type', then join the texts together\n",
    "grouped = instruction_df.groupby(['task_id', 'type'])['instruction'].apply('\\n'.join).reset_index()\n",
    "\n",
    "# Filter out only the 'INSTRUCTION' type\n",
    "instructions = grouped[grouped['type'] == 'INSTRUCTION']\n",
    "\n",
    "new_step_df = new_step_df.merge(instructions[['task_id', 'instruction']], on='task_id', how='left')\n",
    "\n",
    "new_step_df.rename(columns={'task_id': 'id', 'name': 'step_name'}, inplace=True)\n",
    "new_step_df.drop('order_tree', axis=1, inplace=True)\n",
    "new_step_df.drop('checklist_id', axis=1, inplace=True)\n",
    "\n",
    "display(new_step_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.846649284Z",
     "start_time": "2023-10-23T05:44:40.324332052Z"
    }
   },
   "id": "b60271f4d4beb955"
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_step_df.to_sql(table_names['steps'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.846868465Z",
     "start_time": "2023-10-23T05:44:40.369453935Z"
    }
   },
   "id": "634fbf6c8f0ac659"
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "TASK_EXECUTION_QUERY = \"\"\"\n",
    "SELECT te.id AS id, t.id AS task_id, te.reason as reason, TO_TIMESTAMP(te.started_at) AS started_at, TO_TIMESTAMP(te.ended_at) AS ended_at, te.state AS state, te.started_by FROM task_executions te JOIN tasks t ON t.id = te.tasks_id JOIN stages s ON s.id = t.stages_id JOIN checklists c ON c.id = s.checklists_id JOIN jobs j ON j.id = te.jobs_id left join users u ON te.started_by = u.id WHERE t.archived = FALSE AND s.archived = FALSE AND c.id IN %(checklist_ids)s ORDER BY c.id, s.order_tree, t.order_tree\n",
    "\"\"\"\n",
    "task_execution_dtype = {'id': 'Int64', 'task_id': 'Int64', 'started_by': 'Int64'}\n",
    "task_execution_df = pd.read_sql(TASK_EXECUTION_QUERY, source_engine, params=params, dtype=task_execution_dtype)\n",
    "display(task_execution_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.846968341Z",
     "start_time": "2023-10-23T05:44:40.369629276Z"
    }
   },
   "id": "6fc32dd63bbfde87"
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [],
   "source": [
    "PARAMETER_EXECUTION_QUERY = \"\"\"\n",
    "SELECT pv.id, te.id AS task_execution_id, pv.parameters_id, pv.value, pv.choices, p.\"type\" AS parameter_type, to_timestamp(pv.modified_at) AS modified_at, te.tasks_id as task_id FROM parameter_values pv JOIN parameters p ON p.id = pv.parameters_id LEFT JOIN tasks t ON t.id = p.tasks_id LEFT JOIN task_executions te ON te.tasks_id = t.id AND te.jobs_id = pv.jobs_id LEFT JOIN jobs j ON j.id = te.jobs_id AND j.id = pv.jobs_id WHERE p.archived = FALSE AND t.archived = FALSE AND p.type NOT IN ('INSTRUCTION', 'MATERIAL', 'MEDIA', 'SIGNATURE', 'FILE_UPLOAD') AND j.checklists_id IN %(checklist_ids)s\n",
    "\"\"\"\n",
    "parameter_execution_dtype = {'id': 'Int64', 'task_execution_id': 'Int64', 'parameters_id': 'Int64', 'task_id': 'Int64'}\n",
    "parameter_execution_df = pd.read_sql(PARAMETER_EXECUTION_QUERY, source_engine, params=params, dtype=parameter_execution_dtype)\n",
    "display(parameter_execution_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.847063998Z",
     "start_time": "2023-10-23T05:44:40.369730548Z"
    }
   },
   "id": "6f77f64900c647f7"
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [],
   "source": [
    "executed_step_measurement_df = parameter_execution_df.copy()\n",
    "display(parameter_execution_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.847159841Z",
     "start_time": "2023-10-23T05:44:40.413470769Z"
    }
   },
   "id": "e17ace0f5edfd27f"
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6a5116e0a86e3e8d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.847430199Z",
     "start_time": "2023-10-23T05:44:40.413619995Z"
    }
   },
   "outputs": [],
   "source": [
    "new_step_attribute_df = pd.DataFrame(\n",
    "    columns=['id', 'step_id', 'data_type_id', 'attribute_label', 'resource_id', 'expected_value1', 'expected_value2',\n",
    "             'comparison_operator', 'resource_type', 'parameter_id', 'reference_id'])\n",
    "\n",
    "new_step_attribute_data_types_df = pd.DataFrame(\n",
    "    columns=['data_type_id', 'measurement_type', 'measurement_unit', 'measurement_description'])\n",
    "\n",
    "def append_to_attribute_related_df(new_rows_attribute_data_types, new_rows_attribute, attribute_data_types_df, attribute_df):\n",
    "    new_rows_data_types_df = pd.DataFrame(new_rows_attribute_data_types)\n",
    "    new_rows_attribute_df = pd.DataFrame(new_rows_attribute)\n",
    "    attribute_data_types_df = pd.concat([attribute_data_types_df, new_rows_data_types_df], ignore_index=True)\n",
    "    attribute_df = pd.concat([attribute_df, new_rows_attribute_df], ignore_index=True)\n",
    "    return  attribute_data_types_df, attribute_df\n",
    "\n",
    "def create_row_attribute_data_type(data_type_id, measurement_type, measurement_unit, measurement_description):\n",
    "    return {\n",
    "        'data_type_id': data_type_id,\n",
    "        'measurement_type': measurement_type,\n",
    "        'measurement_unit': measurement_unit,\n",
    "        'measurement_description': measurement_description\n",
    "    }\n",
    "\n",
    "def create_row_attribute(attribute_id, parameter_id, step_id, data_type_id, attribute_label, expected_value1, expected_value2, comparison_operator, resource_id, resource_type, reference_id):\n",
    "    return {\n",
    "        'id': attribute_id,\n",
    "        'step_id': step_id,\n",
    "        'data_type_id': data_type_id,\n",
    "        'attribute_label': attribute_label,\n",
    "        'expected_value1': expected_value1,\n",
    "        'expected_value2': expected_value2,\n",
    "        'comparison_operator': comparison_operator,\n",
    "        'resource_id': resource_id,\n",
    "        'resource_type': resource_type,\n",
    "        'parameter_id': parameter_id,\n",
    "        'reference_id': reference_id\n",
    "    }\n",
    "\n",
    "def create_rows_for_attribute_related_data(parameter, identifier):\n",
    "    new_rows_attribute_data_type = []\n",
    "    new_rows_attribute = []\n",
    "    measurement_unit = expected_value1 = expected_value2 = comparison_operator = resource_id = resource_type = reference_id = measurement_type = None\n",
    "    parameter_type, step_id, name = parameter['type'], parameter['task_id'], parameter['name']\n",
    "    if parameter_type not in (['SINGLE_SELECT', 'CHECKLIST', 'MULTISELECT', 'YES_NO']):\n",
    "        is_parameter_type_handled = True\n",
    "        if parameter_type == 'NUMBER':\n",
    "            measurement_type = 'integer'\n",
    "        elif parameter_type == 'SHOULD_BE':\n",
    "            measurement_type = 'float'\n",
    "            operator = parameter['data']['operator']\n",
    "            if operator == 'EQUAL_TO':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '='\n",
    "            elif operator == 'LESS_THAN':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '<'\n",
    "            elif operator == 'LESS_THAN_EQUAL_TO':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '<='\n",
    "            elif operator == 'MORE_THAN':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '>'\n",
    "            elif operator == 'MORE_THAN_EQUAL_TO':\n",
    "                expected_value1 = parameter['data']['value']\n",
    "                comparison_operator = '>='\n",
    "            elif operator == 'BETWEEN':                \n",
    "                expected_value1 = parameter['data']['lowerValue']\n",
    "                expected_value2 = parameter['data']['upperValue']\n",
    "                comparison_operator = 'between'\n",
    "        elif parameter_type == 'SINGLE_LINE' or parameter_type == 'MULTI_LINE':\n",
    "            measurement_type = 'text'\n",
    "        elif parameter_type == 'DATE' or parameter_type == 'DATE_TIME':\n",
    "            measurement_type = parameter_type.lower()\n",
    "        # elif parameter_type == 'YES_NO':\n",
    "        #     measurement_type = 'boolean'\n",
    "        elif parameter_type == 'RESOURCE':\n",
    "            measurement_type = 'text'\n",
    "            resource_type = parameter['data']['collection']\n",
    "        else:\n",
    "            print(f\"Parameter type: {parameter_type} is not implemented\")\n",
    "            is_parameter_type_handled = False\n",
    "        if is_parameter_type_handled:    \n",
    "            new_row_attribute_data_type = create_row_attribute_data_type(identifier, measurement_type, measurement_unit, name)\n",
    "            new_row_attribute = create_row_attribute(identifier, parameter['parameter_id'], step_id, new_row_attribute_data_type['data_type_id'], name, expected_value1, expected_value2, comparison_operator, resource_id, resource_type, reference_id)\n",
    "            new_rows_attribute_data_type.append(new_row_attribute_data_type)\n",
    "            new_rows_attribute.append(new_row_attribute)\n",
    "    else:\n",
    "        if parameter_type in (['SINGLE_SELECT', 'CHECKLIST', 'MULTISELECT', 'YES_NO']):\n",
    "            measurement_type = 'boolean'\n",
    "            for choice in row['data']:\n",
    "                name = parameter['name'] + ' - ' + choice['name']\n",
    "                reference_id = choice['id']\n",
    "                new_row_attribute_data_type = create_row_attribute_data_type(identifier, measurement_type, measurement_unit, name)\n",
    "                new_row_attribute = create_row_attribute(identifier, parameter['parameter_id'], step_id, new_row_attribute_data_type['data_type_id'], name, expected_value1, expected_value2, comparison_operator, resource_id, resource_type, reference_id)\n",
    "                new_rows_attribute_data_type.append(new_row_attribute_data_type)\n",
    "                new_rows_attribute.append(new_row_attribute)\n",
    "                identifier += 1\n",
    "        else:\n",
    "            print(f\"Parameter type: {parameter_type} is not implemented.\")\n",
    "                \n",
    "    return new_rows_attribute_data_type, new_rows_attribute\n",
    "\n",
    "# next_id = get_next_id(new_step_attribute_data_types_df, 'data_type_id')\n",
    "cur = destination_connection.cursor()\n",
    "cur.execute(\"SELECT MAX(data_type_id) FROM step_attribute_data_types\")\n",
    "max_value = cur.fetchone()[0]\n",
    "cur.close()\n",
    "if max_value is None:\n",
    "    max_value = 0\n",
    "next_id = max_value + 1\n",
    "relevant_parameter_df = parameter_df[~parameter_df['type'].isin(['INSTRUCTION', 'MATERIAL', 'MEDIA', 'SIGNATURE', 'FILE_UPLOAD'])]\n",
    "# relevant_parameter_df.rename(columns={'parameter_id' : 'id'}, inplace=True)\n",
    "for index, row in relevant_parameter_df.iterrows():\n",
    "    attribute_data_types, attributes = create_rows_for_attribute_related_data(row, next_id)\n",
    "    if len(attribute_data_types) != 0 and len(attributes) != 0:\n",
    "        new_step_attribute_data_types_df, new_step_attribute_df = append_to_attribute_related_df(attribute_data_types, attributes, new_step_attribute_data_types_df, new_step_attribute_df)\n",
    "    next_id += len(attribute_data_types)\n",
    "\n",
    "# display(new_step_attribute_data_types_df)\n",
    "display(new_step_attribute_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "data": {
      "text/plain": "29"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_step_attribute_data_types_df.to_sql(table_names['step_attribute_data_types'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.847651646Z",
     "start_time": "2023-10-23T05:44:40.510904647Z"
    }
   },
   "id": "587019755e42c011"
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "data": {
      "text/plain": "29"
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_step_attribute_df.to_sql(table_names['step_attributes'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.847970866Z",
     "start_time": "2023-10-23T05:44:40.511064621Z"
    }
   },
   "id": "b2e285ab55369432"
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [],
   "source": [
    "executed_step_df = task_execution_df.copy()\n",
    "executed_step_df.rename(columns={'id': 'execution_id', 'task_id': 'step_id', 'started_at': 'execution_start_time', 'ended_at': 'execution_end_time', 'state': 'status', 'started_by': 'executed_by_employee_id'  }, inplace=True)\n",
    "executed_step_df['batch_id'] = None\n",
    "executed_step_df.drop('reason', axis=1, inplace=True)\n",
    "display(executed_step_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.848119244Z",
     "start_time": "2023-10-23T05:44:40.511186571Z"
    }
   },
   "id": "48804d634e868135"
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "data": {
      "text/plain": "80"
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executed_step_df.to_sql(table_names['executed_steps'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.848337316Z",
     "start_time": "2023-10-23T05:44:40.511298274Z"
    }
   },
   "id": "105cdb66ec2d35cd"
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [],
   "source": [
    "executed_step_exception_df = task_execution_df.copy()\n",
    "executed_step_exception_df = executed_step_exception_df[executed_step_exception_df['state'] == 'COMPLETED_WITH_EXCEPTION']\n",
    "executed_step_exception_df['exception_id'] = executed_step_exception_df['id']\n",
    "executed_step_exception_df.rename(columns={'id': 'execution_id', 'reason': 'description', 'ended_at': 'exception_time'}, inplace=True)\n",
    "executed_step_exception_df.drop('task_id', axis=1, inplace=True)\n",
    "executed_step_exception_df.drop('state', axis=1, inplace=True)\n",
    "executed_step_exception_df.drop('started_at', axis=1, inplace=True)\n",
    "executed_step_exception_df.drop('started_by', axis=1, inplace=True)\n",
    "display(executed_step_exception_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.848437814Z",
     "start_time": "2023-10-23T05:44:40.571719725Z"
    }
   },
   "id": "836979f00f7a3848"
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executed_step_exception_df.to_sql(table_names['executed_step_exceptions'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.848637117Z",
     "start_time": "2023-10-23T05:44:40.571877074Z"
    }
   },
   "id": "882abfab227fcaaf"
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18013/415201650.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  executed_step_measurement_df = pd.concat([executed_step_measurement_df, new_rows_executed_step_measurement_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "executed_step_measurement_df = pd.DataFrame(columns=['execution_id', 'step_attribute_id', 'resource_id', 'measurement_value', 'recorded_time', 'resource_type'])\n",
    "\n",
    "def create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type): \n",
    "    return {\n",
    "        # 'measurement_id': measurement_id,\n",
    "        'execution_id': execution_id,\n",
    "        'step_attribute_id': step_attribute_id,\n",
    "        'resource_id': resource_id,\n",
    "        'measurement_value': measurement_value,\n",
    "        'recorded_time': recorded_time,\n",
    "        'resource_type': resource_type\n",
    "    }    \n",
    "\n",
    "new_rows_executed_step_measurement = []\n",
    "# relevant_parameter_df = parameter_execution_df[~parameter_execution_df['parameter_type'].isin(['INSTRUCTION', 'MATERIAL', 'MEDIA', 'SIGNATURE', 'FILE_UPLOAD'])]\n",
    "for index, row in parameter_execution_df.iterrows():\n",
    "    parameter_type = row['parameter_type']\n",
    "    measurement_id = row['id']\n",
    "    # measurement_id = None\n",
    "    execution_id = row['task_execution_id']\n",
    "    recorded_time = row['modified_at']\n",
    "    step_id = row['task_id']\n",
    "    parameters_id = row['parameters_id']\n",
    "    step_attribute_id = resource_id = measurement_value = resource_type = None\n",
    "    if parameter_type in (['SINGLE_SELECT', 'CHECKLIST', 'MULTISELECT', 'YES_NO', 'RESOURCE']):\n",
    "        if parameter_type == 'RESOURCE':\n",
    "            choices = row['choices']\n",
    "            if choices is not None:\n",
    "                choice = choices[0]\n",
    "                resource_id = choice['objectId']\n",
    "                resource_type = choice['collection']\n",
    "                measurement_value = choice['objectDisplayName'] + ' (ID: ' + choice['objectExternalId'] + ')'\n",
    "                new_row_executed_step_measurement = create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type)\n",
    "                new_rows_executed_step_measurement.append(new_row_executed_step_measurement)\n",
    "        else:\n",
    "            if row['choices'] is not None:\n",
    "                for choice_id, selection in row['choices'].items():\n",
    "                    filtered_series = new_step_attribute_df.loc[(new_step_attribute_df['parameter_id'] == parameters_id) & (new_step_attribute_df['reference_id'] == choice_id)]['id']\n",
    "                    if not filtered_series.empty:\n",
    "                        step_attribute_id = filtered_series.iloc[0]\n",
    "                    else:\n",
    "                        print(f\"Not able to find parameter_id, parameters_id: {parameters_id}, parameter_type: {parameter_type}, row: {row}\")\n",
    "                        step_attribute_id = None\n",
    "                    measurement_value = 'false'\n",
    "                    if selection == 'SELECTED':\n",
    "                        measurement_value = 'true'\n",
    "                    new_row_executed_step_measurement = create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type)\n",
    "                    new_rows_executed_step_measurement.append(new_row_executed_step_measurement)\n",
    "    else:\n",
    "        filtered_series = new_step_attribute_df.loc[(new_step_attribute_df['parameter_id'] == parameters_id)]['id']\n",
    "        if not filtered_series.empty:\n",
    "            step_attribute_id = filtered_series.iloc[0]\n",
    "        else:\n",
    "            print(f\"Not able to find parameter_id, parameters_id: {parameters_id}, parameter_type: {parameter_type}, row: {row}\")\n",
    "            step_attribute_id = None\n",
    "        measurement_value = row['value']\n",
    "        new_row_executed_step_measurement = create_row_executed_step_measurement(measurement_id, execution_id, step_attribute_id, resource_id, measurement_value, recorded_time, resource_type)\n",
    "        new_rows_executed_step_measurement.append(new_row_executed_step_measurement)\n",
    "\n",
    "if len(new_rows_executed_step_measurement) != 0:\n",
    "    new_rows_executed_step_measurement_df = pd.DataFrame(new_rows_executed_step_measurement)\n",
    "    executed_step_measurement_df = pd.concat([executed_step_measurement_df, new_rows_executed_step_measurement_df], ignore_index=True)\n",
    "#             \n",
    "# executed_step_measurement_df\n",
    "# new_rows_executed_step_measurement\n",
    "display(executed_step_measurement_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.848819444Z",
     "start_time": "2023-10-23T05:44:40.571987170Z"
    }
   },
   "id": "347d860a3dbf7242"
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "data": {
      "text/plain": "76"
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executed_step_measurement_df.to_sql(table_names['executed_step_measurements'], destination_engine, if_exists=if_exists, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T05:44:40.849020356Z",
     "start_time": "2023-10-23T05:44:40.625657698Z"
    }
   },
   "id": "e1fdd16355ce7b92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
